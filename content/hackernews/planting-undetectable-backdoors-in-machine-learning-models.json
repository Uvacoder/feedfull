{
  "title": "Planting Undetectable Backdoors in Machine Learning Models",
  "url": "https://arxiv.org/abs/2204.06974",
  "date": "Sun, 17 Apr 2022 21:46:48 +0000",
  "content": "<a href=\"https://news.ycombinator.com/item?id=31064787\">Comments</a>",
  "image": null,
  "description": "Given the computational cost and technical expertise required to train\nmachine learning models, users may delegate the task of learning to a service\nprovider. We show how a malicious learner can plant an undetectable backdoor\ninto a classifier. On the surface, such a backdoored classifier behaves\nnormally, but in reality, the learner maintains a mechanism for changing the\nclassification of any input, with only a slight perturbation. Importantly,\nwithout the appropriate \"backdoor key\", the mechanism is hidden and cannot be\ndetected by any computationally-bounded observer. We demonstrate two frameworks\nfor planting undetectable backdoors, with incomparable guarantees.\n  First, we show how to plant a backdoor in any model, using digital signature\nschemes. The construction guarantees that given black-box access to the\noriginal model and the backdoored version, it is computationally infeasible to\nfind even a single input where they differ. This property implies that the\nbackdoored model has generalization error comparable with the original model.\nSecond, we demonstrate how to insert undetectable backdoors in models trained\nusing the Random Fourier Features (RFF) learning paradigm or in Random ReLU\nnetworks. In this construction, undetectability holds against powerful\nwhite-box distinguishers: given a complete description of the network and the\ntraining data, no efficient distinguisher can guess whether the model is\n\"clean\" or contains a backdoor.\n  Our construction of undetectable backdoors also sheds light on the related\nissue of robustness to adversarial examples. In particular, our construction\ncan produce a classifier that is indistinguishable from an \"adversarially\nrobust\" classifier, but where every input has an adversarial example! In\nsummary, the existence of undetectable backdoors represent a significant\ntheoretical roadblock to certifying adversarial robustness.",
  "publisher": "Hackernews",
  "publisherUrl": "http://news.ycombinator.com/"
}