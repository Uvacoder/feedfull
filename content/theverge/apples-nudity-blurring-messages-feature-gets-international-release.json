{
  "title": "Apple’s nudity-blurring Messages feature gets international release",
  "url": "https://www.theverge.com/2022/4/21/23035183/ios-messages-communication-safety-nudity-sexually-explicit-message-blurring",
  "date": "2022-04-21T09:49:33.000Z",
  "author": "Jon Porter",
  "content": "      <figure>      <img alt=\"\" src=\"https://cdn.vox-cdn.com/thumbor/uDosisxSRBnYwMfeuyrNzylMZeI=/0x0:2100x1400/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70775006/ios15_iphone13_pro_messages_comm_safety_message_choose.0.jpg\" />        <figcaption><em>Warning messages shown to a child when sexually explicit imagery is detected.</em> | Image: Apple</figcaption>    </figure>  <p id=\"RFOgnV\">Apple’s “<a href=\"https://support.apple.com/en-us/HT212850\">communication safety in Messages</a>” feature, which is designed to automatically blur images containing nudity sent to children using the company’s messaging service, is now rolling out to additional countries. After <a href=\"https://www.theverge.com/2021/12/13/22828226/apple-ios-15-2-iphone-update-digital-legacy-music-macro-app-privacy\">launching in the US last year</a>, the feature is now coming to the Messages apps on iOS, iPadOS, and macOS for users in the UK, Canada, New Zealand, and Australia. Exact timings are unclear, but <a href=\"https://www.theguardian.com/technology/2022/apr/20/apple-says-new-child-safety-feature-to-be-rolled-out-for-uk-iphones\"><em>The Guardian </em>reports</a> that the feature is coming to the UK “soon.” </p><p id=\"DpKIX9\">Scanning happens on-device, and does not impact the end-to-end encryption of messages. Instructions on how to enable the feature, which is integrated with Apple’s existing Family Sharing system, can be found <a href=\"https://support.apple.com/en-us/HT212850\">here</a>.</p><p id=\"KJC5z0\">The opt-in feature scans incoming and outgoing pictures for “sexually explicit” material to protect children. If found, the image is blurred and guidance is provided for finding help alongside reassurances that it’s ok not to view the image and to leave the conversation. “You’re not alone, and can always get help from someone you trust or with trained professionals,” reads the pop-up message. “You can also block this person.”</p><div class=\"c-float-right\"><aside id=\"1bs3Yc\"><q>Messaging an adult about an image is optional</q></aside></div><p id=\"u3JAaa\">Similar to its initial release in the US, children will have the option of messaging an adult they trust about a flagged photo. When Apple <a href=\"https://www.theverge.com/2021/8/10/22613225/apple-csam-scanning-messages-child-safety-features-privacy-controversy-explained\">originally announced the feature last August</a>, it suggested that this notification would happen automatically. Critics were quick to point out that this approach risked <a href=\"https://twitter.com/KendraSerra/status/1423365222841135114?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1423367106972852228%7Ctwgr%5E%7Ctwcon%5Es2_&amp;ref_url=https%3A%2F%2Fwww.theverge.com%2F2021%2F8%2F6%2F22613365%2Fapple-icloud-csam-scanning-whatsapp-surveillance-reactions\">outing queer kids</a> to their parents, and could otherwise be abused.</p><p id=\"TefBH7\">Apple is also expanding the rollout of a new feature for Spotlight, Siri, and Safari searches that will point users towards safety resources if they search for topics relating to child sexual abuse. </p><p id=\"x75TrN\">Alongside these two child safety features, Apple originally announced a third initiative last August that involved scanning photos for child sexual abuse material (CSAM) before they’re uploaded to a user’s iCloud account. However, this feature drew intense backlash from privacy advocates, who argued it risked introducing a backdoor that would <a href=\"https://www.theverge.com/2021/8/10/22613225/apple-csam-scanning-messages-child-safety-features-privacy-controversy-explained\">undermine the security of Apple’s users</a>. The company later announced it would <a href=\"https://www.theverge.com/2021/9/3/22655644/apple-delays-controversial-child-protection-features-csam-privacy\">delay the rollout of all three features</a> while it addressed concerns. Having released the first two features, Apple has yet to provide an update on when the more controversial CSAM detection feature will become available.</p>",
  "image": "https://cdn.vox-cdn.com/thumbor/assFcgFXYFJsLcrOlW3Z-xht_6w=/0x151:2100x1250/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/23405789/ios15_iphone13_pro_messages_comm_safety_message_choose.jpg",
  "description": "Designed to protect children from sexually explicit material.",
  "publisher": "The Verge",
  "publisherUrl": "https://www.theverge.com/"
}