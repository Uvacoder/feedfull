{
  "title": "Facebook moderators ‘err on the side of an adult’ when uncertain of age in possible abuse photos",
  "url": "https://www.theverge.com/2022/3/31/23005576/facebook-content-moderators-child-sexual-abuse-material-csam-policy",
  "date": "2022-04-01T00:28:12.000Z",
  "author": "Jay Peters",
  "content": "      <figure>      <img alt=\"facebook stock art\" src=\"https://cdn.vox-cdn.com/thumbor/NrZNMejz3bnvyPJSk_xWNU44180=/0x0:3000x2000/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70696459/acastro_180928_1777_facebook_hack_0001.0.jpg\" />        <figcaption>Illustration by Alex Castro / The Verge</figcaption>    </figure>  <p id=\"6Pq6BE\">A major responsibility for tech companies is to monitor content on their platforms for child sexual abuse material (CSAM), and if any is found, they are legally required to report it to  the National Center for Missing and Exploited Children (NCMEC). Many companies have content moderators in place that review content flagged for potentially being CSAM, and they determine whether the content should be reported to the NCMEC.</p><p id=\"W4W5Sd\">However, Facebook has a policy that could mean it is underreporting child sexual abuse content, according to <a href=\"https://www.nytimes.com/2022/03/31/business/meta-child-sexual-abuse.html\">a new report from <em>The New York Times</em></a>. A Facebook training document directs content moderators to “err on the side of an adult” when they don’t know someone’s age in a photo or video that’s suspected to be CSAM, the report said. </p><p id=\"3jf0zm\">The policy was made for Facebook content moderators working at Accenture and is discussed in <a href=\"https://www.californialawreview.org/print/internet-gov-tech-companies-as-government-agents-and-the-future-of-the-fight-against-child-sexual-abuse/\">a <em>California Law Review</em> article from August</a>: </p><blockquote><p id=\"okzdfw\">Interviewees also described a policy called “bumping up,” which each of them personally disagreed with. The policy applies when a content moderator is unable to readily determine whether the subject in a suspected CSAM photo is a minor (“B”) or an adult (“C”). In such situations, content moderators are instructed to assume the subject is an adult, thereby allowing more images to go unreported to NCMEC.</p></blockquote><p id=\"EScIpv\">Here is the company’s reasoning for the policy, from <em>The New York Times</em>:</p><blockquote><p id=\"gmbFCu\">Antigone Davis, head of safety for Meta, confirmed the policy in an interview and said it stemmed from privacy concerns for those who post sexual imagery of adults. “The sexual abuse of children online is abhorrent,” Ms. Davis said, emphasizing that Meta employs a multilayered, rigorous review process that flags far more images than any other tech company. She said the consequences of erroneously flagging child sexual abuse could be “life-changing” for users.</p></blockquote><p id=\"soCDGd\">When reached for comment, Facebook (which is now under the Meta corporate umbrella) pointed to Davis’ quotes in the <em>NYT</em>. Accenture didn’t immediately reply to a request for comment. Accenture declined to comment to <em>The New York Times</em>.</p><p id=\"WiGrxV\"><em><strong>Update March 31st, 9:09PM ET</strong></em><em>: Facebook pointed to </em>Davis’ <em>quotes in the </em>NYT<em>.</em></p>",
  "image": "https://cdn.vox-cdn.com/thumbor/XvVt25j7b4dci2iG5s3LjXFnEe4=/0x215:3000x1786/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/13177665/acastro_180928_1777_facebook_hack_0001.jpg",
  "description": "The New York Times reported on Facebook’s policy.",
  "publisher": "The Verge",
  "publisherUrl": "https://www.theverge.com/"
}