{
  "title": "Nvidia reveals H100 GPU for AI and teases ‘world’s fastest AI supercomputer’",
  "url": "https://www.theverge.com/2022/3/22/22989182/nvidia-ai-hopper-architecture-h100-gpu-eos-supercomputer",
  "date": "2022-03-22T15:49:07.000Z",
  "author": "James Vincent",
  "content": "      <figure>      <img alt=\"\" src=\"https://cdn.vox-cdn.com/thumbor/TWfi-xvRNVqcbfX89F6_k2aDGIg=/348x128:1242x724/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70657118/Hopper_Arch___H100_Die___Image.0.png\" />        <figcaption><em>The H100 die. </em> | Image: Nvidia</figcaption>    </figure>  <p id=\"Pppdmj\">Nvidia has announced a slew of AI-focused enterprise products at its annual GTC conference. They include <a href=\"https://go.redirectingat.com?id=66960X1514734&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fdata-center%2Fhopper-architecture%2F&amp;referrer=theverge.com&amp;sref=https%3A%2F%2Fwww.theverge.com%2F2022%2F3%2F22%2F22989182%2Fnvidia-ai-hopper-architecture-h100-gpu-eos-supercomputer\" rel=\"sponsored nofollow noopener\" target=\"_blank\">details</a> of its new silicon architecture, Hopper; the first datacenter GPU built using that architecture, <a href=\"https://nvidianews.nvidia.com/news/nvidia-announces-dgx-h100-systems-worlds-most-advanced-enterprise-ai-infrastructure\">the H100</a>; a new <a href=\"https://nvidianews.nvidia.com/news/nvidia-introduces-grace-cpu-superchip\">Grace CPU “superchip”</a>; and vague plans to build what the company claims will be the world’s fastest AI supercomputer, <a href=\"https://nvidianews.nvidia.com/news/nvidia-announces-dgx-h100-systems-worlds-most-advanced-enterprise-ai-infrastructure\">named Eos</a>.</p><p id=\"DisNog\">Nvidia has benefited hugely from the AI boom of the last decade, with its GPUs proving a perfect match for popular, data-intensive deep learning methods. As the AI sector’s demand for data compute grows, says Nvidia, it wants to provide more firepower.</p><p id=\"SuGrgL\">In particular, the company stressed the popularity of a type of machine learning system known as a Transformer. This method has been incredibly fruitful, powering everything from language models like OpenAI’s GPT-3 to medical systems like DeepMind’s AlphaFold. Such models have increased exponentially in size over the space of a few years. When OpenAI launched GPT-2 in 2019, for example, it contained 1.5 billion parameters (or connections). When Google trained a similar model just two years later, it used <a href=\"https://venturebeat.com/2021/01/12/google-trained-a-trillion-parameter-ai-language-model/\">1.6 <em>trillion</em> parameters</a>.</p><div class=\"c-float-right\"><aside id=\"dpEn0s\"><q>As AI demands more computer, Nvidia wants to deliver it</q></aside></div><p id=\"TyLEBF\">“Training these giant models still takes months,” said Nvidia senior director of product management Paresh Kharya in a press briefing. “So you fire a job and wait for one and half months to see what happens. A key challenge to reducing this time to train is that performance gains start to decline as you increase the number of GPUs in a data center.”</p><p id=\"Bsh4ql\">Nvidia says its new Hopper architecture will help ameliorate these difficulties. Named after <a href=\"https://news.harvard.edu/gazette/story/2014/12/grace-hopper-computing-pioneer/\">pioneering computer scientist</a> and US Navy Rear Admiral Grace Hopper, the architecture is specialized to <a href=\"https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/\">accelerate the training of Transformer</a> models on H100 GPUs by six times compared to previous-generation chips, while the new fourth-generation Nivida NVlink can connect up to 256 H100 GPUs at nine times higher bandwidth than the previous generation. </p><p id=\"G1VCi1\">The H100 GPU itself contains 80 billion transistors and is the first GPU to support PCle Gen5 and utilize HBM3, enabling memory bandwidth of 3TB/s. Nvidia says an H100 GPU is three times faster than its previous-generation A100 at FP16, FP32, and FP64 compute, and six times faster at 8-bit floating point math. </p><p id=\"rdvmJF\">“For the training of giant Transformer models, H100 will offer up to nine times higher performance, training in days what used to take weeks,” said Kharya.</p><p id=\"KWQDAg\">The company also announced a new data center CPU, the Grace CPU Superchip, which consists of two CPUs connected directly via a new low-latency NVLink-C2C. The chip is designed to “serve giant-scale HPC and AI applications” alongside the new Hopper-based GPUs, and can be used for CPU-only systems or GPU-accelerated servers. It has 144 Arm cores and 1TB/s of memory bandwidth. </p>  <figure class=\"e-image\">        <img alt=\" \" data-mask-text=\"false\" src=\"https://cdn.vox-cdn.com/thumbor/KEydxnP6sDmNQdnfjgi5Q6MzXVg=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/23337016/Screenshot_2022_03_22_at_16.05.31.png\">      <cite>Image: Nvidia</cite>      <figcaption><em>The new Grace CPU “superchip” consists of two CPUs connected together. </em></figcaption>  </figure><p id=\"oLVfxR\">In addition to hardware and infrastructure news, Nvidia also announced updates to its various enterprise AI software services, including Maxine (an SDK to deliver audio and video enhancements, intended to power things like virtual avatars) and Riva (an SDK  used for both speech recognition and text-to-speech). </p><p id=\"Q1dp8F\">The company also teased that it was building a new AI supercomputer, which it claims will be the world’s fastest when deployed. The supercomputer, named Eos, will be built using the Hopper architecture and contain some 4,600 H100 GPUs to offer 18.4 exaflops of “AI performance.” The system will be used for Nvidia’s internal research only, and the company said it would be online in a few months’ time. </p><p id=\"qkz9gY\">Over the past few years, a number of companies with strong interest in AI have built or announced their own in-house “AI supercomputers” for internal research, including Microsoft, Tesla, and Meta. These systems are not directly comparable with regular supercomputers as they run at a lower level of accuracy, which has allowed a number of firms to quickly leapfrog one another by announcing the world’s fastest.</p><p id=\"1fynOx\">However, during his keynote address, Nvidia CEO Jensen Huang did say that Eos, when running traditional supercomputer tasks, would rack 275 petaFLOPS of compute — 1.4 times faster than “the fastest science computer in the US” (the Summit). “We expect Eos to be the fastest AI computer in the world,” said Huang. “Eos will be the blueprint for the most advanced AI infrastructure for our OEMs and cloud partners.”</p>",
  "image": "https://cdn.vox-cdn.com/thumbor/vFY3B6jdCHkBj4_oX9D000_OFDQ=/348x216:1242x684/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/23336691/Hopper_Arch___H100_Die___Image.png",
  "description": "Nvidia wants to go even bigger with AI.",
  "publisher": "The Verge",
  "publisherUrl": "https://www.theverge.com/"
}